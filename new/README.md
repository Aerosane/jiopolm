# Comprehensive Cognitive Capability Development Framework

## A Systematic Guide to Building Advanced Reasoning Architecture

---

# PART I: FOUNDATIONAL ARCHITECTURE

---

## 1. Executive Summary and Philosophical Foundation

This curriculum represents a structured approach to cognitive development, moving from reliable foundational reasoning to adaptive engineering intelligence to probabilistic epistemic mastery. The framework recognizes that human cognition is not a single faculty but a constellation of interrelated capabilities that must be developed systematically.

### Core Thesis

Cognitive excellence emerges from the deliberate cultivation of three interdependent systems: 

1. **Retrieval-Reconstruction Systems** (Near Level) â€” The ability to reliably access and reassemble knowledge structures
2. **Adaptive Engineering Systems** (Urahara Level) â€” The ability to modify approaches dynamically and recover from failures
3. **Probabilistic-Epistemic Systems** (L Level) â€” The ability to reason under uncertainty while maintaining rigorous constraint satisfaction

### Guiding Principles

| Principle | Implementation | Rationale |
|-----------|----------------|-----------|
| **Cognitive Load Management** | Progressive complexity scaling | Prevents overwhelm; builds automaticity |
| **Desirable Difficulty** | Challenges calibrated to edge of competence | Optimizes encoding and retrieval |
| **Interleaving** | Mixed practice across domains | Develops discrimination and transfer |
| **Elaborative Interrogation** | Constant "why" and "how" questioning | Deepens causal understanding |
| **Metacognitive Monitoring** | Explicit self-assessment checkpoints | Develops calibration and error detection |
| **Spaced Retrieval** | Distributed practice over time | Maximizes long-term retention |
| **Cognitive Economy** | Effort scaled to stakes and payoff | Prevents over-engineering simple problems | <!-- ADDED: Cognitive Economy -->

> **âš¡ Cognitive Economy Principle (NEW):**
> Not every problem deserves maximum cognitive investment. Before engaging deeply, ask: *"What is the cost of being wrong here?  What is the value of being precisely right vs. approximately right?"* Reliable-enough solutions often outperform perfect solutions delivered too late or at excessive cost.

---

## 2. Level Overview and Skill Cluster Definitions

### 2.1 Near Level: Structured Recall + Reconstructive Reasoning

#### Definition
The Near cognitive profile represents **systematic, methodical intelligence** characterized by: 
- Exceptional working memory organization
- Reliable retrieval of stored procedures and facts
- Ability to reconstruct solutions from first principles
- Strong sequential reasoning chains
- Consistent application of known frameworks

#### Cognitive Signature

```
INPUT â†’ Pattern Recognition â†’ Schema Activation â†’ Procedural Execution â†’ Verification â†’ OUTPUT
```

#### Core Competencies

| Competency | Description | Observable Behavior |
|------------|-------------|---------------------|
| **Structured Recall** | Reliable access to stored knowledge | Can reproduce proofs, derivations, procedures from memory |
| **Reconstructive Reasoning** | Building solutions from foundational elements | Derives forgotten formulas from first principles |
| **Sequential Logic** | Step-by-step deductive chains | Maintains coherent argument structure across many steps |
| **Pattern Application** | Matching problems to known solution templates | Quickly identifies problem type and applicable method |
| **Verification Loops** | Systematic checking of work | Catches arithmetic and logical errors consistently |
| **Retrieval Indexing** | Organized access paths to knowledge | Uses named handles ("this is a convergence test") rather than rote search | <!-- ADDED: Memory Indexing -->

<!-- ADDED: Memory Indexing & Retrieval Control annotation -->
> **ðŸ·ï¸ Retrieval Handles (NEW):**
> Near-level mastery is not about memorizing everythingâ€”it's about *knowing how to find what you know*.  Develop named retrieval handles: 
> - "This looks like a **substitution trap**" (named failure mode)
> - "I need the **sandwich lemma pattern**" (technique handle)
> - "This is a **base rate neglect situation**" (bias handle)
>
> *Drill: After solving any problem, assign it a 2-4 word retrieval tag. Build a personal index.*

#### Learning Outcomes for Near Level

Upon completing Near-level training, the learner will: 

1. Execute multi-step problem-solving procedures with >95% accuracy on familiar problem types
2. Reconstruct any learned derivation or proof from first principles within 2x the original learning time
3. Identify the problem category and appropriate solution method within 60 seconds for trained domains
4. Detect and correct their own errors through systematic verification protocols
5. Organize knowledge hierarchically for efficient retrieval
6. **Maintain a lightweight external index of failure modes and technique handles** <!-- ADDED -->

---

### 2.2 Urahara Level: Adaptive Engineering + Error Recovery

#### Definition
The Urahara cognitive profile represents **creative, adaptive intelligence** characterized by:
- Rapid prototype-test-refine cycles
- Comfort with incomplete information
- Ability to repurpose tools and methods across domains
- Graceful degradation under failure
- Strategic misdirection and unconventional approaches

#### Cognitive Signature

```
PROBLEM â†’ Constraint Mapping â†’ Resource Inventory â†’ Prototype Solution â†’
Failure Analysis â†’ Adaptation â†’ Iterate â†’ Novel Synthesis â†’ OUTPUT
```

<!-- ADDED: Error-First Design annotation -->
> **ðŸ”´ Error-First Orientation (NEW):**
> Urahara-level thinking assumes your first model is probably wrong. This is not pessimismâ€”it's engineering realism. Build solutions with: 
> - **Checkpoints**:  "If I'm still on track, X should be true here"
> - **Contingency branches**: "If this fails, my fallback is Y"
> - **Failure detection triggers**: "If I see Z, I've made the sign-error trap"

#### Core Competencies

| Competency | Description | Observable Behavior |
|------------|-------------|---------------------|
| **Adaptive Engineering** | Modifying approaches based on feedback | Changes strategy mid-problem when hitting obstacles |
| **Error Recovery** | Converting failures into information | Uses wrong approaches to eliminate possibilities |
| **Cross-Domain Transfer** | Applying tools from unrelated fields | Sees connections between disparate disciplines |
| **Resource Optimization** | Maximizing output from minimal inputs | Solves problems with limited tools or information |
| **Strategic Ambiguity** | Operating effectively despite uncertainty | Makes progress without complete specifications |
| **Unconventional Framing** | Redefining problems to enable solutions | "That's not a bug, it's a feature" thinking |
| **Contingency Architecture** | Building solutions with fallback paths | Never commits fully to single-track reasoning | <!-- ADDED: Error-First Design -->

#### Learning Outcomes for Urahara Level

Upon completing Urahara-level training, the learner will:

1. Generate at least 3 distinct solution approaches for any well-defined problem
2. Recover from failed solution attempts within 2 iterations using failure-derived information
3. Transfer solution methods across at least 3 different domains
4. Solve problems with deliberately degraded information or tools
5. Identify non-obvious problem reformulations that simplify the solution space
6. **Articulate contingency paths before committing to primary approach** <!-- ADDED -->

---

### 2.3 L Level: Probabilistic, Constraint-Based, Epistemic Intelligence

#### Definition
The L cognitive profile represents **detective-like analytical intelligence** characterized by: 
- Probabilistic reasoning under uncertainty
- Simultaneous constraint satisfaction
- Epistemic humility combined with decisive action
- Adversarial thinking and deception detection
- Minimal-information maximum-inference extraction

#### Cognitive Signature

```
OBSERVATIONS â†’ Hypothesis Generation â†’ Bayesian Updating â†’
Constraint Propagation â†’ Anomaly Detection â†’ Inference Chains â†’
Confidence Calibration â†’ CONCLUSIONS (with uncertainty bounds)
```

#### Core Competencies

| Competency | Description | Observable Behavior |
|------------|-------------|---------------------|
| **Probabilistic Reasoning** | Thinking in likelihood distributions | Maintains multiple hypotheses with probability weights |
| **Constraint Satisfaction** | Handling multiple simultaneous requirements | Finds solutions that satisfy all constraints optimally |
| **Epistemic Tracking** | Knowing what you know and don't know | Explicitly identifies knowledge gaps and assumptions |
| **Adversarial Modeling** | Thinking from opponent's perspective | Anticipates counter-moves and deceptive strategies |
| **Minimal-Information Inference** | Extracting maximum insight from limited data | Draws valid conclusions from sparse observations |
| **Calibration** | Accurate confidence in own judgments | Stated confidence matches actual accuracy |
| **Assumption Surfacing** | Making implicit premises explicit | Lists assumptions before drawing conclusions | <!-- ADDED:  Epistemic Hygiene -->

<!-- ADDED: Epistemic Hygiene Enhancement -->
> **ðŸ§¼ Epistemic Hygiene Protocol (NEW):**
> Before concluding, L-level reasoners explicitly surface: 
> 1. **Assumptions**: "I am assuming X, Y, Z"
> 2. **Confidence level**: "I am 70% confident, meaning I expect to be wrong ~30% of the time"
> 3. **Hypothesis ranking**: "H1 > H2 > H3" (not "H1 is true, H2 is false")
> 4. **Update triggers**: "I would revise if I observed W"
>
> *One-line checkpoint: "What would change my mind?"*

#### Learning Outcomes for L Level

Upon completing L-level training, the learner will:

1. Maintain and update probability distributions over multiple competing hypotheses
2. Solve constraint satisfaction problems with 5+ simultaneous constraints
3. Achieve >80% calibration (when 80% confident, correct 80% of the time)
4. Detect logical and epistemic inconsistencies in presented information
5. Model adversarial agents and anticipate strategic behavior
6. **Explicitly state assumptions and confidence levels before conclusions** <!-- ADDED -->
7. **Rank hypotheses rather than accepting/rejecting binarily** <!-- ADDED -->

---

## 3. Peak Brilliance Elements

Beyond the three core levels, certain cognitive capabilities represent **peak performance** markers:

### 3.1 Integration Markers

| Marker | Description | Assessment Method |
|--------|-------------|-------------------|
| **Fluid Expertise** | Seamless level-switching based on problem demands | Present mixed problems requiring different approaches |
| **Generative Abstraction** | Creating new frameworks from patterns across domains | Novel problem creation and framework synthesis tasks |
| **Predictive Modeling** | Anticipating outcomes before they occur | Forecasting challenges with feedback loops |
| **Cognitive Endurance** | Maintaining performance under extended load | Multi-hour problem sessions with quality tracking |
| **Insight Generation** | Sudden reconceptualization of problem structure | "Aha moment" tracking and conditions analysis |
| **Rule-Space Awareness** | Treating problems as formal systems with exploitable edges | Identifies constraint boundaries and valid loopholes | <!-- ADDED: Rule-Space Awareness -->

<!-- ADDED: Rule-Space & Constraint Awareness section -->
### 3.2 Rule-Space & Constraint Awareness (NEW)

Peak-level cognition recognizes that every problem exists within a **formal rule-space** with: 
- **Explicit constraints**:  Stated rules that must be satisfied
- **Implicit constraints**: Unstated assumptions that are actually flexible
- **Edge cases**: Boundary conditions where rules behave unexpectedly
- **Loopholes**: Valid solutions that satisfy the letter but not the spirit of constraints

**Protocol for Constraint Exploitation:**
```
1. FIRST:  Solve the problem correctly within intended constraints
2. THEN: List all explicit and implicit assumptions
3. THEN: Identify which assumptions are actually required vs. conventional
4. ONLY THEN: Explore whether relaxing assumptions enables better solutions
```

> âš ï¸ **Critical**:  Constraint exploitation is only permitted AFTER correctness is secured.  Premature cleverness creates fragile solutions.

**Optional Drill â€” The Assumption Audit:**
> Take a solved problem.  List every assumption you made (including "obvious" ones like "numbers are real" or "time moves forward"). Ask: which of these are required by the problem statement, and which did I add? 

### 3.3 Meta-Cognitive Excellence

```
LEVEL 1: Monitoring (knowing what you're doing)
    â†“
LEVEL 2: Control (adjusting based on monitoring)
    â†“
LEVEL 3: Meta-Monitoring (knowing how well you're monitoring)
    â†“
LEVEL 4: Strategic Selection (choosing cognitive strategies deliberately)
    â†“
LEVEL 5: Cognitive Architecture Design (building your own thinking systems)
```

<!-- ADDED: Learning-Rate Optimization annotation -->
> **ðŸ“ˆ Learning-Rate Awareness (NEW):**
> Meta-cognitive excellence includes tracking not just *what* you know, but *how fast your models are updating*. After each significant learning experience, ask:
> - "What changed in my understanding?"
> - "How much did my probability estimates shift?"
> - "What used to seem certain that now seems questionable?"
>
> *Progress = speed of model update, not just correctness.*

---

# PART II:  PHASE-WISE FRAMEWORKS

---

## 4. Near Level Development Phases

### Phase N1: Foundation Building (Weeks 1-4)

#### Objectives
- Establish core retrieval pathways
- Build procedural automaticity
- Develop verification habits

#### Daily Structure

| Time Block | Activity | Duration |
|------------|----------|----------|
| Block 1 | Retrieval practice (flashcards, recall) | 30 min |
| Block 2 | Worked example study | 45 min |
| Block 3 | Guided problem solving | 60 min |
| Block 4 | Self-testing and error analysis | 30 min |

#### Progressive Drills

**Week 1-2: Schema Acquisition**
- Study worked examples with self-explanation
- Copy and annotate solution procedures
- Create procedure flowcharts
- Verbal explanation of steps (teach-back method)

**Week 3-4: Retrieval Activation**
- Attempt problems before looking at solutions
- Fill-in-the-blank derivations
- Reconstruct solutions from memory
- Compare reconstruction to original

<!-- ADDED: External Indexing Strategy -->
> **ðŸ“ External Indexing Strategy (NEW):**
> During Phase N1, begin a minimal **Failure Mode Index**:
> - Format: Simple text file or single notebook page
> - Entry template: `[Tag] â€” [Brief description] â€” [How to detect]`
> - Example: `[Sign Trap] â€” Lost negative in step 3 â€” Check:  substitute original values`
> - Rule: Maximum 1 new entry per day (prevents over-documentation)

#### Evaluation Criteria

| Criterion | Novice | Developing | Proficient | Mastery |
|-----------|--------|------------|------------|---------|
| Recall Accuracy | <50% | 50-70% | 70-90% | >90% |
| Procedure Fluency | Many pauses | Some hesitation | Smooth execution | Automatic |
| Error Rate | >20% | 10-20% | 5-10% | <5% |
| Verification Usage | Rarely | Sometimes | Usually | Always |

---

### Phase N2: Consolidation (Weeks 5-8)

#### Objectives
- Increase retrieval reliability
- Expand pattern recognition
- Develop reconstruction capability

#### Refinement Loops

```
ATTEMPT â†’ ERROR ANALYSIS â†’ TARGETED PRACTICE â†’ RE-ATTEMPT â†’
COMPARE â†’ IDENTIFY REMAINING GAPS â†’ ADJUST FOCUS â†’ REPEAT
```

<!-- ADDED: Mid-Solution Verification Checkpoint protocol -->
> **âœ“ Mid-Solution Verification Protocol (NEW):**
> At the halfway point of any multi-step problem, pause and ask:
> 1. "Does my intermediate result have the right units/type/sign?"
> 2. "Does this match my intuition about magnitude?"
> 3. "If I plug in a simple test case, does it work?"
>
> *This catches errors when recovery cost is still low.*

#### Drill Progression

**Week 5-6: Interleaved Practice**
- Mix problem types within practice sessions
- Identify problem type before solving
- Explicit categorization exercises
- Cross-reference between similar types

**Week 7-8: Reconstruction Training**
- Solve problems without formula sheets
- Derive formulas from first principles
- "Regeneration" exercises:  recreate proofs from memory
- Stress testing:  time-pressure retrieval

---

### Phase N3: Mastery Integration (Weeks 9-12)

#### Objectives
- Achieve automatic retrieval
- Handle increased complexity
- Develop teaching capability

#### Capstone Activities

1. **Teaching Sessions**:  Explain concepts to others (real or imagined)
2. **Comprehensive Problem Sets**: Multi-step problems requiring multiple schemas
3. **Novel Recombinations**: Problems requiring combination of learned techniques
4. **Stress Testing**: Timed comprehensive assessments

<!-- ADDED: Cognitive Economy checkpoint -->
> **âš¡ Cognitive Economy Checkpoint (NEW):**
> Before capstone problems, practice the **Effort Calibration Question**:
> *"Is this a situation requiring 95% accuracy or 80% accuracy?  Am I over-investing?"*
>
> For low-stakes practice:  aim for speed and good-enough. 
> For high-stakes assessment: invest fully. 

---

## 5. Urahara Level Development Phases

### Phase U1: Flexibility Introduction (Weeks 1-4)

#### Objectives
- Break rigid procedural thinking
- Introduce multiple solution paths
- Develop comfort with ambiguity

#### Core Activities

| Activity | Purpose | Frequency |
|----------|---------|-----------|
| **Multi-Path Analysis** | See same problem 3+ ways | Daily |
| **Constraint Removal** | "What if X weren't true?" | 3x/week |
| **Tool Limitation** | Solve without standard tools | 2x/week |
| **Failure Journaling** | Document and analyze failures | After each session |

<!-- ADDED: Named Failure Mode practice -->
> **ðŸ·ï¸ Failure Mode Naming Drill (NEW):**
> When documenting failures, always assign a **named tag**:
> - âŒ "I made an error" (too vague)
> - âœ“ "I fell into the **Overfitting Trap** â€” optimized for specific case, failed on general"
> - âœ“ "This was a **Premature Commitment** â€” locked into approach before surveying options"
>
> Building a vocabulary of named failures accelerates pattern recognition.

#### Progressive Drills

**Week 1-2: Alternative Path Discovery**
- Solve problem one way, then find another
- Study multiple textbooks' approaches
- "There's always another way" mindset development

**Week 3-4: Deliberate Failure Practice**
- Attempt problems you don't know how to solve
- Document what stopped you
- Analyze failure patterns
- Convert failures to learning targets

---

### Phase U2: Adaptive Mastery (Weeks 5-8)

#### Objectives
- Develop rapid pivoting capability
- Build cross-domain transfer
- Strengthen error recovery

#### Drill Structures

**The Pivot Drill**
```
1. Start solving with Method A
2. Instructor says "BLOCKED" at random point
3. Must find alternative path from current state
4. Continue until solution or exhaustion
5. Analyze:  What made pivoting hard?  Easy? 
```

**The Transplant Drill**
```
1. Learn technique in Domain X
2. Given novel problem in Domain Y
3. Identify abstract structure shared with X
4. Apply X-technique in Y-context
5. Document transfer conditions
```

<!-- ADDED: Contingency Path requirement -->
> **ðŸ”€ Contingency Path Requirement (NEW):**
> Before executing any solution in Phase U2+, articulate:
> - **Primary path**: "I will try X"
> - **Trigger condition**: "If I see Y, this path is failing"
> - **Fallback path**: "My backup is Z"
>
> Single-track reasoning is now considered incomplete. 

---

### Phase U3: Engineering Integration (Weeks 9-12)

#### Objectives
- Achieve fluid adaptation
- Handle ill-defined problems
- Develop strategic resource management

#### Capstone Challenges

1. **The Black Box**:  Solve a problem with hidden constraints revealed incrementally
2. **The Junkyard**: Solve using only specified (limited) tools
3. **The Sabotage**: Solution path deliberately broken mid-way; must recover
4. **The Chimera**: Problem requiring synthesis of 3+ domain techniques

<!-- ADDED:  Assumption Listing protocol -->
> **ðŸ“‹ Pre-Exploitation Assumption Audit (NEW):**
> For capstone challenges, before attempting any unconventional solution:
> 1.  Solve the problem conventionally first (establish correctness baseline)
> 2. List all assumptions (explicit and implicit)
> 3. Mark which assumptions are required by problem statement vs. added by you
> 4. Only then explore whether relaxing your additions enables better solutions
>
> *Cleverness without correctness is fragility.*

---

## 6. L Level Development Phases

### Phase L1: Probabilistic Foundations (Weeks 1-4)

#### Objectives
- Develop Bayesian intuition
- Build hypothesis management skills
- Establish calibration habits

#### Core Activities

| Activity | Purpose | Frequency |
|----------|---------|-----------|
| **Calibration Training** | State confidence, track accuracy | Every problem |
| **Prior Elicitation** | Estimate before calculating | Daily |
| **Likelihood Comparison** | Which hypothesis explains data better?  | Daily |
| **Surprise Logging** | Record when reality differs from expectation | Continuous |

<!-- ADDED: Epistemic Hygiene checkpoint -->
> **ðŸ§¼ Epistemic Hygiene Checkpoint (NEW):**
> For every conclusion in Phase L1+, complete this micro-template:
> - **Confidence**:  ___% (must be a number)
> - **Key assumption**: _____________
> - **Would revise if**: _____________
>
> This takes 10 seconds and prevents overconfidence.

#### Progressive Drills

**Week 1-2: Confidence Calibration**
- Answer questions with stated confidence
- Track accuracy at each confidence level
- Adjust based on feedback
- Target: calibration curve matching identity line

<!-- ADDED:  Hypothesis Ranking practice -->
> **ðŸ“Š Hypothesis Ranking Practice (NEW):**
> Never say "I believe X." Instead, practice: 
> - "My current ranking: H1 (60%) > H2 (25%) > H3 (15%)"
> - After new evidence:  "Updated ranking: H2 (50%) > H1 (35%) > H3 (15%)"
>
> Binary belief ("X is true/false") is banned at L level.  Everything is ranked.

**Week 3-4: Bayesian Updating**
- Present evidence, update beliefs
- Explicit prior â†’ likelihood â†’ posterior chains
- Probability wheels and distributions
- Verbal reasoning about relative probabilities

---

### Phase L2: Constraint Intelligence (Weeks 5-8)

#### Objectives
- Handle simultaneous constraints
- Develop constraint propagation intuition
- Master elimination reasoning

#### Drill Structures

**The Constraint Cascade**
```
1. Given: Multiple constraints (5-10)
2. Task: Find all satisfying solutions
3. Method:  Propagate constraints, eliminate impossibilities
4. Track: Which constraints were most informative?
5. Analyze:  Optimal ordering for constraint application
```

**The Logical Detective**
```
1. Given: Scenario with hidden information
2. Given: Set of observations
3. Task: Determine hidden information
4. Track:  Explicit inference chains
5. Evaluate: Was conclusion justified by evidence?
```

<!-- ADDED:  Assumption Surfacing requirement -->
> **ðŸ“ Mandatory Assumption Surfacing (NEW):**
> Before any constraint satisfaction problem, write:
> - "I am assuming the following are true:  [list]"
> - "I am assuming the following are NOT true: [list]"
> - "I am assuming the following remain constant: [list]"
>
> Hidden assumptions are the #1 source of constraint satisfaction errors.

---

### Phase L3: Epistemic Mastery (Weeks 9-12)

#### Objectives
- Achieve integrated probabilistic-constraint reasoning
- Master adversarial thinking
- Develop meta-epistemic awareness

#### Capstone Challenges

1. **The Deception Game**: Detect when information is deliberately misleading
2. **The Sparse Signal**: Maximum inference from minimal observations
3. **The Moving Target**: Adversary actively countering your reasoning
4. **The Meta-Puzzle**: Reason about the puzzle-creator's intentions

<!-- ADDED:  Learning-Rate Reflection -->
> **ðŸ“ˆ Learning-Rate Reflection (NEW):**
> After each capstone, answer:
> 1. "What probability estimates changed most during this problem?"
> 2. "What took longer to update than it should have?"
> 3. "What should I update faster next time?"
>
> Meta-learning: learning how to learn faster.

---

# PART III:  PROBLEM SETS

---

## 7. Near Level Problem Sets

### Problem N1: The Derivation Reconstruction

**Problem Statement**

Without looking at any references, derive the quadratic formula starting from the general quadratic equation axÂ² + bx + c = 0. Show every algebraic step and explain the reasoning for each transformation.

**Learning Intent**

This problem trains: 
- Procedural recall of completing the square
- Systematic algebraic manipulation
- Self-explanation and verification habits
- Reconstruction from first principles

**Mid-Solution Checkpoint Question**

*After completing the square on the left side, pause and ask yourself:*
- What does the expression inside the squared term look like?
- Why did we add (b/2a)Â² to both sides?
- Can I verify this step by expanding the squared term? 

<!-- ADDED: Assumption surfacing prompt -->
> **ðŸ§¼ Assumption Check (NEW):** Before proceeding, state:  "I am assuming a â‰  0." (Why? What breaks if a = 0?)

**Expected Pitfalls**

| Pitfall | Detection Method | Recovery | Retrieval Tag | <!-- ADDED column -->
|---------|------------------|----------|---------------|
| Forgetting to factor out 'a' first | Squared term doesn't simplify cleanly | Go back; divide entire equation by 'a' | **Leading Coefficient Trap** |
| Sign errors in (b/2a)Â² | Expansion check doesn't match original | Carefully re-track negative signs | **Sign Propagation Error** |
| Algebraic errors in simplification | Result doesn't match known formula | Verify each step by substitution | **Arithmetic Drift** |
| Skipping the Â± in square root | Only one solution found | Remember xÂ² = k implies x = Â±âˆšk | **Root Completeness Miss** |

**Step-by-Step Solution**

```
Step 1: Start with general form
axÂ² + bx + c = 0

Step 2: Divide everything by 'a' to make leading coefficient 1
xÂ² + (b/a)x + c/a = 0

Step 3: Move constant to right side
xÂ² + (b/a)x = -c/a

Step 4: Complete the square
- Take half of the coefficient of x:  (b/a)/2 = b/(2a)
- Square it: (b/(2a))Â² = bÂ²/(4aÂ²)
- Add to both sides: 
xÂ² + (b/a)x + bÂ²/(4aÂ²) = -c/a + bÂ²/(4aÂ²)

Step 5: Left side is now a perfect square
(x + b/(2a))Â² = -c/a + bÂ²/(4aÂ²)

Step 6:  Simplify right side (common denominator 4aÂ²)
(x + b/(2a))Â² = -4ac/(4aÂ²) + bÂ²/(4aÂ²) = (bÂ² - 4ac)/(4aÂ²)

Step 7: Take square root of both sides (Â±)
x + b/(2a) = Â±âˆš(bÂ² - 4ac)/(2a)

Step 8: Solve for x
x = -b/(2a) Â± âˆš(bÂ² - 4ac)/(2a) = (-b Â± âˆš(bÂ² - 4ac))/(2a)

Verification: Substitute back into original equation to confirm. 
```

**Reflection Template**

```markdown
## Post-Problem Reflection:  N1

### Recall Assessment
- [ ] I completed the derivation without looking at references
- [ ] I remembered all key steps
- Steps I forgot or struggled with:  ________________

### Understanding Assessment
- [ ] I can explain WHY we complete the square
- [ ] I understand each algebraic transformation
- Transformations that felt mechanical rather than understood: ________________

### Error Analysis
- Errors I made: ________________
- How I caught them: ________________
- Root cause: ________________
- **Retrieval tag assigned**: ________________ <!-- ADDED -->

### Efficiency Assessment
- Time taken: ______ minutes
- Time spent on dead ends: ______ minutes
- How to be faster next time: ________________

### Consolidation
- What I will review: ________________
- Spaced repetition schedule: ________________

### Assumption Audit <!-- ADDED -->
- Assumptions I made: ________________
- Which were required vs. added by me: ________________
```

---

### Problem N2: The Multi-Step Physics Derivation

**Problem Statement**

Derive the formula for the range of a projectile launched at angle Î¸ with initial velocity vâ‚€ on level ground. Start from Newton's second law and constant acceleration kinematics.  Assume no air resistance. 

**Learning Intent**

This problem trains: 
- Chain reasoning across multiple physical principles
- Coordinate system setup
- Algebraic manipulation with trigonometric functions
- Connecting abstract math to physical situation

**Mid-Solution Checkpoint Question**

*After finding the time of flight, pause and ask yourself:*
- Does this time expression make physical sense?  (e.g., doubles if vâ‚€ doubles)
- What happens at Î¸ = 90Â°? Does my expression predict this correctly?
- Am I tracking my variables consistently?

<!-- ADDED: Mid-solution verification prompt -->
> **âœ“ Verification Checkpoint (NEW):** Check units at this point.  Time should have units of [length]/[acceleration]^(1/2) = seconds. Does your expression pass? 

**Expected Pitfalls**

| Pitfall | Detection Method | Recovery | Retrieval Tag |
|---------|------------------|----------|---------------|
| Incorrect coordinate decomposition | sin/cos swapped | Redraw diagram; vertical uses sin for typical angle definition | **Trig Decomposition Swap** |
| Forgetting factor of 2 in time of flight | Final range off by factor of 2 | Time up = time down; total = 2Ã—time_up | **Symmetry Miss** |
| Sign convention errors | Negative distances appearing | Establish convention at start; be consistent | **Sign Convention Drift** |
| Not using identity 2sinÎ¸cosÎ¸ = sin(2Î¸) | Unsimplified final expression | Apply trig identities at end | **Identity Blindness** |

**Step-by-Step Solution**

```
Step 1: Set up coordinate system
- x:  horizontal (positive in direction of launch)
- y: vertical (positive upward)
- Origin:  launch point

Step 2:  Decompose initial velocity
- vâ‚€â‚“ = vâ‚€ cos(Î¸) [horizontal component]
- vâ‚€áµ§ = vâ‚€ sin(Î¸) [vertical component]

Step 3: Apply kinematics equations
Horizontal: x(t) = vâ‚€â‚“ t = vâ‚€ cos(Î¸) t [no horizontal acceleration]
Vertical: y(t) = vâ‚€áµ§ t - Â½gtÂ² = vâ‚€ sin(Î¸) t - Â½gtÂ²

Step 4: Find time of flight
Projectile lands when y(t) = 0:
vâ‚€ sin(Î¸) t - Â½gtÂ² = 0
t(vâ‚€ sin(Î¸) - Â½gt) = 0
t = 0 (launch) or t = 2vâ‚€ sin(Î¸)/g (landing)

Time of flight: T = 2vâ‚€ sin(Î¸)/g

Step 5: Find range (horizontal distance at landing)
R = vâ‚€ cos(Î¸) Ã— T
R = vâ‚€ cos(Î¸) Ã— 2vâ‚€ sin(Î¸)/g
R = 2vâ‚€Â² sin(Î¸)cos(Î¸)/g

Step 6: Apply trigonometric identity
2 sin(Î¸)cos(Î¸) = sin(2Î¸)

R = vâ‚€Â² sin(2Î¸)/g

Step 7: Verify limiting cases
- Î¸ = 0Â°: R = 0 âœ“ (horizontal launch at ground level)
- Î¸ = 45Â°: R = vâ‚€Â²/g (maximum range) âœ“
- Î¸ = 90Â°: R = 0 âœ“ (vertical launch)
```

---

### Problem N3: The Logic Chain

**Problem Statement**

Prove that âˆš2 is irrational using proof by contradiction. Present the proof with explicit logical structure, stating each inference rule used. 

**Learning Intent**

This problem trains: 
- Proof by contradiction structure
- Logical inference tracking
- Definition application (rational, irrational)
- Verification of logical completeness

**Mid-Solution Checkpoint Question**

*After assuming âˆš2 = p/q in lowest terms, pause and ask yourself:*
- What are ALL the properties I'm assuming about p and q?
- What would it mean to "derive a contradiction"?
- What properties of even and odd numbers might be relevant?

<!-- ADDED: Assumption surfacing requirement -->
> **ðŸ“‹ Explicit Assumption List (NEW):**
> Before proceeding, write down: 
> 1. "âˆš2 is rational" (assumption for contradiction)
> 2. "p, q are integers with q â‰  0"
> 3. "gcd(p, q) = 1" (lowest terms)
> 4. "If nÂ² is even, then n is even" (lemmaâ€”do you need to prove this?)

**Expected Pitfalls**

| Pitfall | Detection Method | Recovery | Retrieval Tag |
|---------|------------------|----------|---------------|
| Not specifying "lowest terms" | Contradiction doesn't materialize | Must assume gcd(p,q) = 1 | **Missing Constraint Specification** |
| Leap in "pÂ² even implies p even" | Proof has gap | Prove this lemma explicitly or cite it | **Unstated Lemma Dependence** |
| Losing track of the contradiction | Ending without clear contradiction statement | Explicitly state:  "We assumed X, but showed not-X" | **Contradiction Articulation Fail** |
| Circular reasoning | Using irrationality to prove irrationality | Trace every claim to definitions and axioms | **Circularity Trap** |

**Step-by-Step Solution**

```
Theorem: âˆš2 is irrational. 

Proof Structure:  Proof by contradiction
- Assume the negation of what we want to prove
- Derive a logical contradiction
- Conclude the original statement must be true

Step 1: Assume âˆš2 is rational [for contradiction]
By definition of rational:  âˆš2 = p/q where p, q are integers, q â‰  0
Additional assumption: p/q is in lowest terms, meaning gcd(p, q) = 1

Step 2: Square both sides
2 = pÂ²/qÂ²
pÂ² = 2qÂ² ...  (*)

Step 3: Analyze parity of p
From (*): pÂ² is even (since pÂ² = 2 Ã— (something))
Lemma: If nÂ² is even, then n is even
  Proof of lemma: If n is odd, then n = 2k+1 for some integer k
  Then nÂ² = 4kÂ² + 4k + 1 = 2(2kÂ² + 2k) + 1, which is odd
  Contrapositive: nÂ² even implies n even âˆŽ

Therefore: p is even
So: p = 2m for some integer m

Step 4: Substitute back into (*)
(2m)Â² = 2qÂ²
4mÂ² = 2qÂ²
2mÂ² = qÂ²

Step 5: Analyze parity of q
From above: qÂ² = 2mÂ², so qÂ² is even
By the same lemma: q is even

Step 6: Derive contradiction
We have shown:  p is even AND q is even
This means: 2 divides both p and q
Therefore: gcd(p, q) â‰¥ 2

But we assumed:  gcd(p, q) = 1

Contradiction: gcd(p, q) â‰¥ 2 AND gcd(p, q) = 1 cannot both be true

Step 7: Conclude
Our assumption that âˆš2 is rational leads to contradiction
Therefore: âˆš2 is irrational âˆŽ
```

---

## 8. Urahara Level Problem Sets

### Problem U1: The Constrained Engineering Challenge

**Problem Statement**

You need to measure exactly 4 liters of water, but you only have two unmarked containers:  one holds exactly 3 liters when full, and one holds exactly 5 liters when full. You have an unlimited water source and a drain. Find a solution, and then find at least two MORE distinct solutions.

**Learning Intent**

This problem trains:
- Multiple solution path generation
- State-space exploration
- Constraint satisfaction under resource limitations
- Abstract structure recognition (this is essentially modular arithmetic)

**Mid-Solution Checkpoint Question**

*After finding your first solution, pause and ask yourself:*
- What is the "state" of this system?  (How do I fully describe where I am?)
- How many possible states exist? 
- Is there a pattern to what amounts I can create? 
- Could I work BACKWARD from the goal state? 

<!-- ADDED:  Contingency path prompt -->
> **ðŸ”€ Before You Start (NEW):** Articulate your contingency:  "If forward exploration doesn't yield results within 5 minutes, I will try backward chaining from the goal state."

**Expected Pitfalls**

| Pitfall | Detection Method | Recovery | Retrieval Tag |
|---------|------------------|----------|---------------|
| Stopping at first solution | Haven't found 3 solutions | Explicitly ask "What else could work?" | **Premature Satisfaction** |
| Missing the "pour until full/empty" insight | Stuck in inefficient exploration | Key operation is pour Aâ†’B until A empty OR B full | **Operation Vocabulary Gap** |
| Not tracking state systematically | Repeating states, going in circles | Draw state transition diagram | **State Amnesia** |
| Thinking only forward | Missing efficient backward paths | Start from (_, 4) and ask "What led here?" | **Unidirectional Bias** |

**Step-by-Step Solution**

```
State representation: (amount in 3L, amount in 5L)
Goal:  Reach any state with 4 in either container

SOLUTION 1: Fill 5, pour to 3, empty 3, pour remainder to 3, fill 5, pour to 3
(0,0) â†’ fill 5L â†’ (0,5) â†’ pour 5Lâ†’3L until 3L full â†’ (3,2)
â†’ empty 3L â†’ (0,2) â†’ pour 5Lâ†’3L â†’ (2,0)
â†’ fill 5L â†’ (2,5) â†’ pour 5Lâ†’3L until 3L full â†’ (3,4) âœ“

Operations:  Fill(5), Pour(5â†’3), Empty(3), Pour(5â†’3), Fill(5), Pour(5â†’3)
Total: 6 operations

SOLUTION 2: Fill 3, pour to 5, fill 3, pour to 5, empty 5, pour to 5, fill 3, pour to 5
(0,0) â†’ fill 3L â†’ (3,0) â†’ pour 3Lâ†’5L â†’ (0,3)
â†’ fill 3L â†’ (3,3) â†’ pour 3Lâ†’5L until 5L full â†’ (1,5)
â†’ empty 5L â†’ (1,0) â†’ pour 3Lâ†’5L â†’ (0,1)
â†’ fill 3L â†’ (3,1) â†’ pour 3Lâ†’5L â†’ (0,4) âœ“

Operations: 8 operations

SOLUTION 3: Variation starting with filling 5L twice
(0,0) â†’ fill 5L â†’ (0,5) â†’ pour to 3L â†’ (3,2) â†’ empty 3L â†’ (0,2)
â†’ pour to 3L â†’ (2,0) â†’ fill 5L â†’ (2,5) â†’ pour to 3L â†’ (3,4) âœ“

This is same as Solution 1 (restructured presentation)

TRUE SOLUTION 3: Work from different initial approach
(0,0) â†’ fill 3L â†’ (3,0) â†’ pour to 5L â†’ (0,3) â†’ fill 3L â†’ (3,3)
â†’ pour to 5L â†’ (1,5) â†’ empty 5L â†’ (1,0) â†’ pour to 5L â†’ (0,1)
â†’ fill 3L â†’ (3,1) â†’ pour to 5L â†’ (0,4) âœ“

META-INSIGHT:  The key mathematical structure is: 
- We can create any amount of form 3a - 5b or 5a - 3b where coefficients are non-negative integers
- Since gcd(3,5) = 1, we can create any integer amount
- 4 = 5(2) - 3(2) = 10 - 6 (Solution 1 logic)
- 4 = 3(3) - 5(1) = 9 - 5 (Solution 2 logic)
```

**Reflection Template**

```markdown
## Post-Problem Reflection:  U1

### Solution Diversity Assessment
- [ ] I found at least 3 distinct solutions
- [ ] I can explain what makes each solution "different"
- Solutions I found:  ________________

### Strategic Assessment
- First solution method: ________________
- How I found additional solutions: ________________
- Did I use forward search, backward search, or both?  ________________
- **Contingency path I had prepared**: ________________ <!-- ADDED -->
- **Did I need to use it?**: ________________ <!-- ADDED -->

### Abstraction Assessment
- [ ] I recognized the underlying mathematical structure
- [ ] I can state the general principle for what amounts are achievable
- Abstract insight: ________________

### Transfer Potential
- Similar problems this strategy would solve: ________________
- Domain where this approach might apply: ________________

### Assumption Audit <!-- ADDED -->
- Implicit assumptions I made (e.g., "can only pour, not estimate"): ________________
- Which were required vs.  self-imposed: ________________
```

---

### Problem U2: The Debugging Challenge

**Problem Statement**

A student claims to have proven that all horses are the same color using mathematical induction: 

*Base case*: In any set of 1 horse, all horses are the same color.  âœ“

*Inductive step*:  Assume any set of k horses are all the same color. Consider a set of k+1 horses. Remove one horse (call it horse A). The remaining k horses are the same color by hypothesis. Now put horse A back and remove a different horse (call it horse B). The remaining k horses (including A) are the same color by hypothesis. Since both groups share k-1 horses in common, and those common horses are the same color in both groups, horses A and B must also be the same color.  Therefore all k+1 horses are the same color. âˆŽ

Find the error and explain precisely why the proof fails.

**Learning Intent**

This problem trains:
- Error detection in logical arguments
- Careful analysis of proof structure
- Finding edge cases that break general arguments
- Distinguishing "usually works" from "always works"

**Mid-Solution Checkpoint Question**

*After reading the proof, pause and ask yourself:*
- The argument structure seems valid.  So where could it go wrong?
- Does the argument work for ALL values of k, or might it fail for some specific k?
- What happens in the smallest non-trivial case? 

<!-- ADDED: Error-First orientation prompt -->
> **ðŸ”´ Error-First Mindset (NEW):** The conclusion is absurd, so the proof MUST be wrong. Your job is not to evaluate whether it's wrong, but to locate exactly where.  Treat this as a debugging task: the bug exists; find it.

**Expected Pitfalls**

| Pitfall | Detection Method | Recovery | Retrieval Tag |
|---------|------------------|----------|---------------|
| Accepting the proof as valid | Conclusion is obviously false | False conclusion means proof has error somewhere | **Absurdity Blindness** |
| Vague identification of error | Cannot state precisely what fails | Be specific:  which step, which case | **Imprecise Diagnosis** |
| Wrong identification | Error claimed in valid step | Test each step with concrete small example | **Misattribution** |
| Thinking base case is wrong | Base case IS actually valid | Base case for n=1 is fine; look elsewhere | **Wrong Target** |

**Step-by-Step Solution**

```
ERROR IDENTIFICATION: 

The proof fails in the inductive step when k = 1. 

DETAILED ANALYSIS: 

For the inductive step to work, when we have k+1 horses: 
- We remove horse A, leaving k horses (same color by hypothesis)
- We remove horse B, leaving k horses (same color by hypothesis)
- The argument claims these two groups SHARE k-1 horses

The sharing claim requires:  k + k = (k+1) + (k-1)
This means the two groups of k horses must overlap in k-1 horses. 

CHECK THE CRITICAL CASE:  k = 1

When k = 1, we have k + 1 = 2 horses:  {Horse A, Horse B}
- Remove horse A: we have {Horse B} - 1 horse, same color âœ“
- Remove horse B: we have {Horse A} - 1 horse, same color âœ“
- Overlap: k - 1 = 0 horses

There is NO overlap!  The two groups don't share any horses! 
Therefore, we CANNOT conclude horses A and B are the same color.

WHY THIS BREAKS THE INDUCTION: 

Induction requires:  Base case âŸ¹ P(2) âŸ¹ P(3) âŸ¹ ... 

The chain breaks at the very first step: 
- P(1) is true (any single horse is same color as itself)
- P(1) âŸ¹ P(2) FAILS (the "overlap" argument doesn't work)
- Even though P(k) âŸ¹ P(k+1) works for k â‰¥ 2, we never GET to k = 2

GENERALIZATION: 

This is an example of an inductive proof that fails at the first step.
The inductive step proof technique is valid for k â‰¥ 2, but since we can't
establish P(2), the entire induction chain never starts. 

LESSON:  Always verify the inductive step works for the smallest case
where it needs to apply (k = base case value).
```

<!-- ADDED: Named failure mode -->
> **ðŸ·ï¸ Retrieval Tag:  "Base Case Boundary Failure"** â€” The inductive step assumes structural properties that don't hold at the boundary between base case and first inductive application.

---

### Problem U3: The Resource Limitation Challenge

**Problem Statement**

You are a researcher who needs to determine which of 12 visually identical components is defective (either heavier or lighter than the restâ€”you don't know which). You have a balance scale that compares weights but gives no numerical readings. You can only use the scale 3 times total. Design a procedure that guarantees finding the defective component AND determining whether it's heavier or lighter. 

**Learning Intent**

This problem trains: 
- Information-theoretic reasoning
- Decision tree construction
- Worst-case optimization
- Systematic case enumeration

**Mid-Solution Checkpoint Question**

*After your first attempt, pause and ask yourself:*
- How many possible "answers" are there?  (12 items Ã— 2 possibilities = 24)
- How much information does each weighing provide? (3 outcomes:  L, R, Balance)
- What's the maximum information I can extract from 3 weighings?  (3Â³ = 27)
- Is the problem solvable in principle? (27 > 24, so yes, barely)

<!-- ADDED: Cognitive economy check -->
> **âš¡ Effort Check (NEW):** This problem requires exhaustive case analysis. Before diving in, confirm: is this a situation where "good enough" works, or do you need the guaranteed-complete solution?  (Answer: problem demands guarantee, so full investment is warranted.)

**Expected Pitfalls**

| Pitfall | Detection Method | Recovery | Retrieval Tag |
|---------|------------------|----------|---------------|
| Dividing into 6 vs 6 on first weigh | Balanced result gives no information | Divide into 4-4-4 to extract maximum info | **Information Waste** |
| Not tracking "could be heavy" vs "could be light" | Lose track of possibilities | Label balls by their remaining possible states | **State Conflation** |
| Trying to solve without writing decision tree | Missing cases | Exhaustively enumerate all paths | **Mental Overload Hubris** |
| Forgetting to determine heavy/light | Only finds which ball | Problem requires BOTH pieces of information | **Incomplete Goal Representation** |

**Step-by-Step Solution**

```
INFORMATION-THEORETIC ANALYSIS: 

Possible states:  24 (any of 12 balls could be defective, and it could be heavy or light)
Information per weighing: logâ‚‚(3) â‰ˆ 1.585 bits (three outcomes)
Information from 3 weighings:  3 Ã— logâ‚‚(3) = logâ‚‚(27) â‰ˆ 4.755 bits
Information needed:  logâ‚‚(24) â‰ˆ 4.585 bits

Since 27 > 24, it's theoretically possible but with very little slack.

SOLUTION PROCEDURE:

Label balls 1-12.

WEIGHING 1: Put balls 1,2,3,4 on LEFT; balls 5,6,7,8 on RIGHT; balls 9,10,11,12 aside

CASE A: LEFT = RIGHT (balanced)
  Defective ball is among {9,10,11,12}
  These balls are of unknown direction (could be heavy or light)

  WEIGHING 2 (Case A): Put balls 1,2,3 on LEFT; balls 9,10,11 on RIGHT

  CASE A. 1: LEFT = RIGHT (balanced)
    Ball 12 is defective
    WEIGHING 3 (A.1): Ball 12 vs Ball 1 (known good)
    â†’ 12 heavy:  Ball 12 is heavy
    â†’ 12 light: Ball 12 is light

  CASE A. 2: LEFT > RIGHT (left heavier, right lighter)
    One of {9,10,11} is LIGHT
    WEIGHING 3 (A.2): Ball 9 vs Ball 10
    â†’ 9 < 10: Ball 9 is light
    â†’ 9 > 10: Ball 10 is light
    â†’ 9 = 10: Ball 11 is light

  CASE A. 3: LEFT < RIGHT
    One of {9,10,11} is HEAVY
    WEIGHING 3 (A.3): Ball 9 vs Ball 10
    â†’ 9 > 10: Ball 9 is heavy
    â†’ 9 < 10: Ball 10 is heavy
    â†’ 9 = 10: Ball 11 is heavy

CASE B: LEFT > RIGHT (1,2,3,4 heavier side)
  Either one of {1,2,3,4} is HEAVY or one of {5,6,7,8} is LIGHT
  Label:  1,2,3,4 are "potentially heavy" (PH)
         5,6,7,8 are "potentially light" (PL)

  WEIGHING 2 (Case B): Put 1,2,5 on LEFT; 3,6,9 on RIGHT
  (Using ball 9 as known good)

  CASE B.1: LEFT = RIGHT (balanced)
    Defective is among {4,7,8}
    4 is PH; 7,8 are PL
    WEIGHING 3 (B.1): Ball 7 vs Ball 8
    â†’ 7 < 8: Ball 7 is light
    â†’ 7 > 8: Ball 8 is light
    â†’ 7 = 8: Ball 4 is heavy

  CASE B.2: LEFT > RIGHT
    Either 1 or 2 is heavy, OR 6 is light
    WEIGHING 3 (B.2): Ball 1 vs Ball 2
    â†’ 1 > 2: Ball 1 is heavy
    â†’ 1 < 2: Ball 2 is heavy
    â†’ 1 = 2: Ball 6 is light

  CASE B.3: LEFT < RIGHT
    Either 3 is heavy OR 5 is light
    WEIGHING 3 (B.3): Ball 3 vs Ball 9 (known good)
    â†’ 3 > 9: Ball 3 is heavy
    â†’ 3 < 9: Ball 5 is light
    â†’ 3 = 9: Impossible (contradictionâ€”Loss case, verify logic)

CASE C: LEFT < RIGHT (1,2,3,4 lighter side)
  Symmetric to Case B; swap "heavy" and "light" throughout
  [Full enumeration follows same pattern]

VERIFICATION:
- Each of 24 possible states leads to unique leaf in decision tree âœ“
- No more than 3 weighings on any path âœ“
- Complete:  all paths terminate with definite answer âœ“
```

---

## 9. L Level Problem Sets

### Problem L1: The Bayesian Detective

**Problem Statement**

A diagnostic test for a rare disease is 99% accurate in both directions: 
- If you have the disease, the test returns positive 99% of the time
- If you don't have the disease, the test returns negative 99% of the time

The disease affects 1 in 10,000 people in the general population.  You test positive.  What is the probability you actually have the disease? 

After calculating, your doctor orders a second independent test (same accuracy), and it also comes back positive. Now what is the probability you have the disease?

**Learning Intent**

This problem trains:
- Bayesian reasoning and base rate incorporation
- Overcoming the "prosecutor's fallacy"
- Sequential updating of probabilities
- Calibrating intuition against calculation

**Mid-Solution Checkpoint Question**

*Before calculating, pause and state your intuitive guess.*
- What do you FEEL the probability is? 
- Write it down:  _______%

*This is important:  most people's intuition is wildly miscalibrated here.*

<!-- ADDED: Hypothesis ranking requirement -->
> **ðŸ“Š Hypothesis Ranking (NEW):** Frame this as ranking, not binary: 
> - H1: I have the disease
> - H2: I don't have the disease
> Before any evidence:  H2 >> H1 (how much?)
> After positive test: Update ranking. 

**Expected Pitfalls**

| Pitfall | Detection Method | Recovery | Retrieval Tag |
|---------|------------------|----------|---------------|
| Ignoring base rate | Answer â‰ˆ 99% | The prior probability matters enormously | **Base Rate Neglect** |
| Confusing P(+\|D) with P(D\|+) | These are VERY different | Always write out the full Bayes formula | **Conditional Inversion** |
| Forgetting false positives dominate | Not accounting for many more non-diseased people | Calculate absolute numbers for intuition | **Denominator Blindness** |
| Incorrect second update | Using wrong prior | Second test prior = first test posterior | **Update Chain Error** |

**Step-by-Step Solution**

```
NOTATION:
- D = has disease
- Â¬D = does not have disease
- + = tests positive
- - = tests negative

GIVEN: 
- P(D) = 1/10,000 = 0.0001 (prior probability of disease)
- P(+|D) = 0.99 (sensitivity)
- P(-|Â¬D) = 0.99, so P(+|Â¬D) = 0.01 (false positive rate)

PART 1: Probability after one positive test

Apply Bayes' Theorem:
P(D|+) = P(+|D) Ã— P(D) / P(+)

First, calculate P(+) using law of total probability: 
P(+) = P(+|D) Ã— P(D) + P(+|Â¬D) Ã— P(Â¬D)
P(+) = 0.99 Ã— 0.0001 + 0.01 Ã— 0.9999
P(+) = 0.000099 + 0.009999
P(+) = 0.010098

Now apply Bayes: 
P(D|+) = (0.99 Ã— 0.0001) / 0.010098
P(D|+) = 0.000099 / 0.010098
P(D|+) â‰ˆ 0.0098 = 0.98%

INTUITION CHECK (using frequency format):
In 1,000,000 people:
- 100 have the disease
  - 99 of these test positive (true positives)
- 999,900 don't have the disease
  - 9,999 of these test positive (false positives)

Total positives: 99 + 9,999 = 10,098
Probability positive is true case: 99 / 10,098 â‰ˆ 0.98%

PART 2: Probability after two positive tests

The posterior from test 1 becomes the prior for test 2:
- P(D) [new] = 0.0098 (approximately 1/102)
- P(Â¬D) [new] = 0.9902

Apply Bayes again:
P(+â‚‚) = P(+|D) Ã— P(D) + P(+|Â¬D) Ã— P(Â¬D)
P(+â‚‚) = 0.99 Ã— 0.0098 + 0.01 Ã— 0.9902
P(+â‚‚) = 0.009702 + 0.009902
P(+â‚‚) = 0.019604

P(D|+â‚,+â‚‚) = (0.99 Ã— 0.0098) / 0.019604
P(D|+â‚,+â‚‚) = 0.009702 / 0.019604
P(D|+â‚,+â‚‚) â‰ˆ 0.495 = 49.5%

ALTERNATIVE CALCULATION (using likelihood ratios):
Prior odds: 1 : 9999
Likelihood ratio per positive test: P(+|D)/P(+|Â¬D) = 99/1 = 99

After one test: (1/9999) Ã— 99 = 99/9999 â‰ˆ 1/101
After two tests: (99/9999) Ã— 99 = 9801/9999 â‰ˆ 0.98 â‰ˆ 1: 1 odds

This confirms:  after two positive tests, probability â‰ˆ 49.5%

METACOGNITIVE NOTE:
- Initial intuition (most people): ~99% after first test, ~99. 99% after second
- Actual:  ~1% after first test, ~50% after second
- The base rate fallacy is one of the most robust cognitive biases
```

**Reflection Template**

```markdown
## Post-Problem Reflection: L1

### Calibration Assessment
- My intuitive guess: ______%
- Calculated answer: ______%
- Gap: _______ percentage points
- [ ] My intuition was well-calibrated
- [ ] I fell for the base rate fallacy

### Conceptual Understanding
- [ ] I understand why the base rate matters so much
- [ ] I can explain the result to a non-mathematician
- My explanation: ________________

### Procedural Fluency
- [ ] I can execute Bayes' theorem without errors
- [ ] I know when to use likelihood ratios instead
- Error-prone steps for me: ________________

### Future Calibration
- In similar problems, I will remember to:  ________________
- Red flags that should trigger "check the base rate": ________________

### Learning-Rate Reflection <!-- ADDED -->
- How much did my intuition update? ________________
- Will I make this error again? (Be honest) ________________
- What would need to happen for this lesson to stick? ________________
```

---

### Problem L2: The Constraint Logic Puzzle

**Problem Statement**

Five philosophers (Aristotle, Berkeley, Confucius, Descartes, Epicurus) are seated around a circular table for a symposium. Using the following clues, determine the seating arrangement (clockwise order):

1. Aristotle sits directly across from Confucius
2. Berkeley is not adjacent to Aristotle
3. Descartes sits immediately to the left of Epicurus
4. Berkeley sits two seats away from Descartes (with one person between them)
5. Confucius is adjacent to Epicurus

**Learning Intent**

This problem trains: 
- Constraint propagation
- Logical elimination
- Circular arrangement reasoning
- Systematic case enumeration

**Mid-Solution Checkpoint Question**

*After placing the first few philosophers, pause and ask yourself:*
- How many independent "anchor" decisions did I make vs. forced placements?
- Am I tracking which clues I've used and which remain to verify?
- Is there a most constraining constraint to start with?

<!-- ADDED: Assumption surfacing requirement -->
> **ðŸ“‹ Assumption Surfacing (NEW):** Before solving, explicitly list: 
> - "Directly across" in a 5-person circle means:  ________________
> - "Immediately to the left" means (clockwise or counter-clockwise): ________________
> - These interpretations are:  [required by problem / my assumption]

**Expected Pitfalls**

| Pitfall | Detection Method | Recovery | Retrieval Tag |
|---------|------------------|----------|---------------|
| Not understanding "circular" arrangement | Treating as linear | "Directly across" in 5 seats means 2 seats apart | **Geometry Mismodel** |
| Inconsistent left/right direction | Constraints conflict | Fix "clockwise" perspective at the start | **Reference Frame Drift** |
| Not checking all constraints at end | Solution violates a clue | Verify every constraint with final answer | **Incomplete Verification** |
| Getting lost in cases | Incomplete or redundant exploration | Draw constraint graph, enumerate systematically | **Case Explosion Overwhelm** |

**Step-by-Step Solution**

```
PRELIMINARY:  Interpret constraints for 5-person circular table

IMPORTANT: In a 5-seat circle, "directly across" is ambiguous since there's no
exact opposite.  Most common interpretation: 2 seats away (as far as possible).

ASSUMPTION AUDIT:
1. "Directly across" = 2 seats apart (the farthest possible in 5 seats)
2. "Immediately to the left" = next seat counter-clockwise (or previous clockwise)
3. Seats numbered 1-5 clockwise

SOLVING: 

Step 1: Apply constraint 3 (most rigid - creates a block)
Descartes immediately LEFT of Epicurus â†’ [D][E] as adjacent pair (D counter-clockwise from E)
Equivalently, going clockwise:  E follows D

Step 2: Apply constraint 5
Confucius adjacent to Epicurus. 
E already has D on one side.
So C must be on E's other side:  [D][E][C] block

Step 3: Try placements of [D][E][C] block
Let's place D at position 1:
- Position 1: D
- Position 2: E
- Position 3: C

Step 4: Apply constraint 1
A "directly across" from C (position 3)
In 5-seat circle, 2 seats from position 3 = positions 1 or 5
Position 1 = D (taken)
So A at position 5

Step 5: Apply constraint 4
B two seats from D (position 1) with one person between
That's position 3 or position 4 (going either direction around)
Position 3 = C (taken)
So B at position 4

Current arrangement (clockwise): 1=D, 2=E, 3=C, 4=B, 5=A

Step 6: Verify all constraints

âœ“ Constraint 1: A (pos 5) across from C (pos 3)
  Distance: 5â†’1â†’2â†’3 = 3 steps, or 5â†’4â†’3 = 2 steps
  Shortest = 2 seats.  âœ“

âœ“ Constraint 2: B (pos 4) not adjacent to A (pos 5)
  Positions 4 and 5 ARE adjacent.  âœ—

# Cognitive Capability Framework â€” Part 2 (Continuation)

---

## 9. L Level Problem Sets (Continued)

### Problem L2: The Constraint Logic Puzzle (Continued)

**Step-by-Step Solution (Continued)**

```
BACKTRACK:  Position 1=D, 2=E, 3=C, 4=B, 5=A violates constraint 2.

Let's try different placement of [D][E][C] block. 

ATTEMPT 2: D at position 2
- Position 2: D
- Position 3: E
- Position 4: C

Apply constraint 1: A across from C (position 4)
2 seats from position 4 = positions 2 or 1
Position 2 = D (taken)
So A at position 1

Apply constraint 4: B two seats from D (position 2)
That's position 4 or position 5
Position 4 = C (taken)
So B at position 5

Current:  1=A, 2=D, 3=E, 4=C, 5=B

Verify constraint 2: B (pos 5) not adjacent to A (pos 1)
Positions 5 and 1 ARE adjacent in a circle.  âœ—

BACKTRACK again. 

ATTEMPT 3: D at position 3
- Position 3: D
- Position 4: E
- Position 5: C

Apply constraint 1: A across from C (position 5)
2 seats from position 5 = positions 2 or 3
Position 3 = D (taken)
So A at position 2

Apply constraint 4: B two seats from D (position 3)
That's position 5 or position 1
Position 5 = C (taken)
So B at position 1

Current:  1=B, 2=A, 3=D, 4=E, 5=C

Verify constraint 2: B (pos 1) not adjacent to A (pos 2)
Positions 1 and 2 ARE adjacent.  âœ—

BACKTRACK again. 

ATTEMPT 4: D at position 4
- Position 4: D
- Position 5: E
- Position 1: C

Apply constraint 1: A across from C (position 1)
2 seats from position 1 = positions 3 or 4
Position 4 = D (taken)
So A at position 3

Apply constraint 4: B two seats from D (position 4)
That's position 1 or position 2
Position 1 = C (taken)
So B at position 2

Current:  1=C, 2=B, 3=A, 4=D, 5=E

Verify constraint 2: B (pos 2) not adjacent to A (pos 3)
Positions 2 and 3 ARE adjacent. âœ—

BACKTRACK again. 

ATTEMPT 5: D at position 5
- Position 5: D
- Position 1: E
- Position 2: C

Apply constraint 1: A across from C (position 2)
2 seats from position 2 = positions 4 or 5
Position 5 = D (taken)
So A at position 4

Apply constraint 4: B two seats from D (position 5)
That's position 2 or position 3
Position 2 = C (taken)
So B at position 3

Current:  1=E, 2=C, 3=B, 4=A, 5=D

Verify ALL constraints: 

âœ“ Constraint 1: A (pos 4) across from C (pos 2)
  Distance 4â†’5â†’1â†’2 = 3 steps, or 4â†’3â†’2 = 2 steps
  Shortest = 2 seats. âœ“

âœ“ Constraint 2: B (pos 3) not adjacent to A (pos 4)
  Positions 3 and 4 ARE adjacent. âœ—

ALL ROTATIONS OF [D][E][C] FAIL CONSTRAINT 2.

RECONSIDERING:  Perhaps "directly across" means something different,
OR there's an error in the puzzle, OR we need to reconsider constraint 5.

ALTERNATIVE INTERPRETATION: 
What if constraint 5 means C is adjacent to E, but on the OTHER side? 
Original: [D][E][C] (C to the right of E)
Alternative: [C][D][E] (C to the left of D, which is left of E)

But waitâ€”in [C][D][E], is C adjacent to E?
C at position n, D at position n+1, E at position n+2
C and E are 2 apart, NOT adjacent.  This violates constraint 5.

So [D][E][C] is the only valid block.

RESOLUTION: The puzzle as stated may be over-constrained for a 5-seat
circular arrangement with standard interpretations.

POSSIBLE SOLUTIONS:
1. Reinterpret "directly across" as "not adjacent" (weaker condition)
2. Accept that the puzzle has no solution under standard interpretation
3. Check if there's a 6-seat interpretation

ATTEMPTING with "across" = "not adjacent" (at least 2 seats apart):

With [D][E][C] at positions 5,1,2:
A must be "not adjacent" to C (position 2), so A at position 4 or 5
Position 5 = D (taken), so A at position 4
B at remaining position 3

Arrangement: 1=E, 2=C, 3=B, 4=A, 5=D

Check constraint 2: B (3) not adjacent to A (4)? They ARE adjacent. âœ—

This interpretation also fails.

FINAL RESOLUTION: 
After exhaustive search, this puzzle has NO VALID SOLUTION under
standard interpretations.  This is itself an important meta-lesson: 
sometimes constraint systems are unsatisfiable. 

LEARNING POINT:  The ability to recognize an unsatisfiable constraint
system is a valuable L-level skill. Not all puzzles have solutions. 
```

<!-- ADDED: Meta-lesson callout -->
> **ðŸ§  Meta-Lesson (NEW):** This problem demonstrates that constraint systems can be **unsatisfiable**.  Recognizing this is an L-level skill.  The correct answer is sometimes "no solution exists under these constraints" rather than forcing an invalid answer. 

**Reflection Template**

```markdown
## Post-Problem Reflection:  L2

### Process Assessment
- [ ] I systematically enumerated cases
- [ ] I tracked which constraints were satisfied at each step
- [ ] I recognized when to backtrack
- Cases I explored: ________________

### Constraint Analysis
- Most constraining constraint:  ________________
- Order I applied constraints: ________________
- Would a different order have been more efficient? ________________

### Assumption Audit <!-- ADDED -->
- Assumptions I made about ambiguous terms: ________________
- [ ] I explicitly stated these before solving
- How did my assumptions affect the solution space? ________________

### Meta-Cognitive Assessment
- [ ] I recognized the possibility of no solution
- [ ] I verified exhaustively before concluding unsatisfiability
- Confidence in "no solution" conclusion: ____%

### Learning-Rate Reflection <!-- ADDED -->
- What changed in my approach to constraint problems? ________________
- New heuristic I will use: ________________
```

---

### Problem L3: The Adversarial Reasoning Challenge

**Problem Statement**

You are playing a game against an adversary.  There are three boxes:  one contains a prize, two are empty.  You choose a box but don't open it. The adversary, who knows where the prize is, must then open one of the remaining boxes that is empty (they cannot open the box with the prize or your chosen box). After they open an empty box, you may switch your choice to the other unopened box or stay with your original choice. 

What is the optimal strategy, and what is your probability of winning with that strategy?  Prove your answer.

**Learning Intent**

This problem trains:
- Conditional probability reasoning
- Adversarial modeling
- Overcoming intuitive biases (Monty Hall problem)
- Rigorous probabilistic proof

**Mid-Solution Checkpoint Question**

*Before calculating, pause and consider:*
- Does the adversary's action give you information? 
- What cases should you condition on?
- What is your intuition telling you, and should you trust it?

<!-- ADDED:  Epistemic hygiene checkpoint -->
> **ðŸ§¼ Epistemic Checkpoint (NEW):**
> State your initial beliefs: 
> - "My intuition says switching wins with probability:  ____%"
> - "My intuition says staying wins with probability: ____%"
> - "Confidence in my intuition: ____%"
>
> Write these down BEFORE calculating.

**Expected Pitfalls**

| Pitfall | Detection Method | Recovery | Retrieval Tag |
|---------|------------------|----------|---------------|
| Thinking probabilities reset to 50-50 | Ignoring information from adversary's action | Trace information flow carefully | **Information Neglect** |
| Not conditioning on adversary's knowledge | Treating as random reveal | Adversary's action is deterministic given their knowledge | **Adversary Model Failure** |
| Intuition override | "It feels like 50-50" | Trust the math, verify with simulation or enumeration | **Intuition Anchoring** |
| Incomplete case analysis | Missing some prize locations | Enumerate all 3 starting positions | **Case Omission** |

**Step-by-Step Solution**

```
SETUP: 
- 3 boxes: A, B, C
- Prize equally likely to be in any box (prior:  1/3 each)
- You choose a box (WLOG, say box A)
- Adversary opens an empty box from {B, C}
- You decide:  stay with A or switch to the other unopened box

ANALYSIS BY CASE ENUMERATION:

Let's trace all possibilities: 

CASE 1: Prize is in A (probability 1/3)
- You chose A (correct)
- Adversary opens B or C (both empty, they pick arbitrarily)
- If you STAY: You WIN
- If you SWITCH: You LOSE

CASE 2: Prize is in B (probability 1/3)
- You chose A (incorrect)
- Adversary MUST open C (B has prize, can't open it)
- If you STAY: You LOSE (A is empty)
- If you SWITCH: You WIN (switch to B)

CASE 3: Prize is in C (probability 1/3)
- You chose A (incorrect)
- Adversary MUST open B (C has prize, can't open it)
- If you STAY: You LOSE (A is empty)
- If you SWITCH: You WIN (switch to C)

SUMMARY: 

| Prize Location | P(location) | Stay Result | Switch Result |
|---------------|-------------|-------------|---------------|
| A             | 1/3         | WIN         | LOSE          |
| B             | 1/3         | LOSE        | WIN           |
| C             | 1/3         | LOSE        | WIN           |

P(Win | Stay) = 1/3
P(Win | Switch) = 2/3

OPTIMAL STRATEGY: Always switch. 
PROBABILITY OF WINNING: 2/3

INTUITION REPAIR: 

Why does this work? Key insight:
- Your initial choice is correct with probability 1/3
- The adversary's reveal doesn't change this! 
- The adversary's reveal DOES concentrate the remaining 2/3 probability
  onto the single other unopened box

Think of it this way: 
- Initially:  1/3 in your box, 2/3 in "the other two boxes combined"
- After reveal: 1/3 still in your box, 2/3 now entirely in the one remaining box
- The adversary's action transfers probability, not creates it

BAYESIAN VERIFICATION: 

P(Prize in A | Adversary opened B) = ?

Using Bayes: 
P(A | opened B) = P(opened B | A) Ã— P(A) / P(opened B)

P(opened B | A) = 1/2 (adversary picks randomly between B and C)
P(opened B | B) = 0 (can't open box with prize)
P(opened B | C) = 1 (must open B, since C has prize)

P(opened B) = P(opened B | A)P(A) + P(opened B | B)P(B) + P(opened B | C)P(C)
            = (1/2)(1/3) + (0)(1/3) + (1)(1/3)
            = 1/6 + 0 + 1/3 = 1/2

P(A | opened B) = (1/2 Ã— 1/3) / (1/2) = 1/3 âœ“
P(C | opened B) = (1 Ã— 1/3) / (1/2) = 2/3 âœ“

Switching doubles your probability of winning. 
```

<!-- ADDED: Learning-rate reflection prompt -->
> **ðŸ“ˆ Update Check (NEW):** If your intuition said 50%, you just updated by ~17 percentage points. This is one of the largest intuition-vs-calculation gaps in probability.  Remember this feeling of surpriseâ€”it calibrates future skepticism of probabilistic intuitions.

**Reflection Template**

```markdown
## Post-Problem Reflection: L3

### Calibration Assessment
- My intuitive guess for switching:  ____%
- Actual probability: 66.7%
- Gap: _____ percentage points
- [ ] I fell for the 50-50 intuition trap

### Reasoning Assessment
- [ ] I enumerated all cases correctly
- [ ] I understood why the adversary's action provides information
- [ ] I can explain this to someone else convincingly
- My one-sentence explanation: ________________

### Adversarial Modeling
- [ ] I correctly modeled that the adversary MUST open an empty box
- [ ] I understood how their constraint affects probabilities
- What I learned about modeling adversaries: ________________

### Learning-Rate Assessment <!-- ADDED -->
- How much did my probability estimate update? ________________
- How confident am I that I won't make this error again? ____%
- What would make this lesson permanent? ________________

### Transfer <!-- ADDED -->
- Other situations where revealed information seems irrelevant but isn't:  ________________
```

---

# PART IV: DOMAIN MAPPING AND META-REASONING

---

## 10. Mapping to Real-World Domains

### 10.1 Mathematics

| Level | Cognitive Focus | Example Activities | Key Pitfalls |
|-------|-----------------|-------------------|--------------|
| **Near** | Proof reconstruction, formula derivation | Reproduce proofs from memory; derive identities | Rote execution without understanding |
| **Urahara** | Multiple proof strategies, conjecture formation | Find 3 different proofs of same theorem | Fixation on first approach |
| **L** | Probabilistic existence arguments, constraint satisfaction | Estimate likelihood of conjecture truth; satisfiability problems | Overconfidence in unverified intuitions |

<!-- ADDED: Domain-specific retrieval tags -->
> **ðŸ·ï¸ Math Retrieval Tags (NEW):**
> - **Existence Trap**:  Proving something exists without constructing it
> - **Uniqueness Miss**: Proving existence but forgetting uniqueness
> - **Edge Case Blindness**: Proof works for nâ‰¥2 but not n=1
> - **Notational Drift**: Symbols change meaning mid-proof

### 10.2 Physics

| Level | Cognitive Focus | Example Activities | Key Pitfalls |
|-------|-----------------|-------------------|--------------|
| **Near** | Equation application, unit analysis | Solve textbook problems; dimensional verification | Plugging without physical reasoning |
| **Urahara** | Approximation strategies, limiting cases | Fermi estimation; extreme parameter analysis | Over-precision when approximation suffices |
| **L** | Model selection, uncertainty propagation | Compare competing theories; error analysis | False precision; ignoring model limitations |

<!-- ADDED:  Cognitive economy note -->
> **âš¡ Physics Economy Note (NEW):** Most physics problems don't require exact solutions. Practice asking: "Do I need 3 significant figures or will 1 suffice?" Order-of-magnitude reasoning is often more valuable than precise calculation.

### 10.3 Logic and Philosophy

| Level | Cognitive Focus | Example Activities | Key Pitfalls |
|-------|-----------------|-------------------|--------------|
| **Near** | Formal proof systems, valid inference | Truth tables; natural deduction | Missing edge cases in case analysis |
| **Urahara** | Paradox resolution, framework comparison | Analyze logical paradoxes; compare ethical frameworks | Forcing resolution when paradox is genuine |
| **L** | Epistemic logic, multi-agent reasoning | Model what agents know about what others know | Conflating knowledge levels |

### 10.4 Engineering

| Level | Cognitive Focus | Example Activities | Key Pitfalls |
|-------|-----------------|-------------------|--------------|
| **Near** | Standard design patterns, specification compliance | Implement known solutions; verify requirements | Over-engineering simple problems |
| **Urahara** | Rapid prototyping, failure-mode analysis | Build-test-iterate cycles; FMEA | Premature optimization |
| **L** | Risk assessment, adversarial robustness | Threat modeling; fault-tolerant design | Ignoring tail risks |

<!-- ADDED:  Error-first engineering note -->
> **ðŸ”´ Engineering Error-First Principle (NEW):** Good engineering assumes components will fail.  For every design, ask: "What fails first?  What happens when it does?  How will I know?" Design detection and recovery, not just prevention. 

### 10.5 Research

| Level | Cognitive Focus | Example Activities | Key Pitfalls |
|-------|-----------------|-------------------|--------------|
| **Near** | Literature review, method replication | Reproduce published results; systematic review | Missing relevant prior work |
| **Urahara** | Experimental design, novel methods | Design studies with control conditions; method adaptation | Confirmation bias in design |
| **L** | Hypothesis generation, statistical inference | Bayesian analysis; meta-analysis | p-hacking; publication bias blindness |

<!-- ADDED:  Assumption surfacing for research -->
> **ðŸ“‹ Research Assumption Protocol (NEW):**
> Before any study, explicitly list:
> 1. "I assume the following is true:  [theoretical framework]"
> 2. "I assume my measurements capture:  [constructs]"
> 3. "I assume the following confounds are absent: [list]"
> 4. "I would revise my hypothesis if I observed: [specific outcomes]"

---

## 11. Meta-Reasoning Rubrics

### 11.1 Output Quality Evaluation

Use this rubric to assess the quality of your own reasoning outputs:

| Dimension | Poor (1) | Developing (2) | Proficient (3) | Excellent (4) |
|-----------|----------|----------------|----------------|---------------|
| **Correctness** | Contains errors that affect conclusion | Minor errors that don't affect conclusion | Correct with minor verification gaps | Correct with complete verification |
| **Completeness** | Major cases missing | Some cases missing | All cases addressed, some tersely | All cases thoroughly addressed |
| **Clarity** | Logic hard to follow | Logic followable with effort | Logic clear to informed reader | Logic clear to any reader |
| **Efficiency** | Excessive steps or wasted effort | Some unnecessary work | Reasonably direct path | Elegant, minimal path |
| **Robustness** | Breaks with small changes | Handles some variations | Handles most variations | Explicitly addresses edge cases |

<!-- ADDED:  Calibration dimension -->
| **Calibration** | Confidence unrelated to accuracy | Weak correlation | Good correlation | Explicit uncertainty bounds that match actual accuracy |

### 11.2 Common Cognitive Traps

| Trap | Description | Detection | Prevention |
|------|-------------|-----------|------------|
| **Confirmation Bias** | Seeking evidence for existing belief | Notice if you're ignoring contradictory data | Actively seek disconfirmation |
| **Anchoring** | Over-weighting first information | Notice if estimates cluster near initial value | Generate multiple independent estimates |
| **Availability** | Overweighting easily recalled examples | Notice if examples are vivid rather than representative | Seek base rates and statistics |
| **Overconfidence** | Excessive certainty in judgments | Track calibration over time | Assign explicit probability ranges |
| **Sunk Cost** | Continuing failed approaches | Notice reluctance to abandon investment | Set explicit abandonment criteria in advance |
| **Planning Fallacy** | Underestimating time/resources | Compare estimates to actual outcomes | Use reference class forecasting |

<!-- ADDED: Trap tagging system -->
> **ðŸ·ï¸ Trap Tagging (NEW):** When you catch yourself in a cognitive trap, tag it:
> - "That was **Anchoring**â€”I started at X and didn't move enough"
> - "That was **Confirmation Bias**â€”I only looked for evidence supporting my hypothesis"
>
> Building a tagged history of your traps reveals personal patterns. 

### 11.3 Self-Verification Protocols

**Level 1: Basic Verification (Near)**
```
â–¡ Did I answer the question that was asked?
â–¡ Are my calculations arithmetically correct?
â–¡ Does my answer have the right units/type?
â–¡ Does my answer pass sanity checks (sign, magnitude)?
```

**Level 2: Structural Verification (Urahara)**
```
â–¡ Did I consider alternative approaches?
â–¡ Does my solution work for edge cases?
â–¡ If my method failed, do I have a backup? 
â–¡ Have I identified the weakest link in my reasoning?
```

**Level 3: Epistemic Verification (L)**
```
â–¡ Have I stated my assumptions explicitly?
â–¡ Have I quantified my uncertainty? 
â–¡ Have I considered adversarial/deceptive scenarios? 
â–¡ What evidence would change my conclusion?
â–¡ Is my confidence calibrated to my actual accuracy?
```

<!-- ADDED: Assumption audit integration -->
> **ðŸ“‹ Universal Assumption Audit (NEW):**
> Add to every verification level:
> - "Assumptions I made: ________________"
> - "Which were required vs. self-imposed:  ________________"
> - "Which could I relax to find better solutions: ________________"

---

# PART V:  CURRICULUM ROADMAP AND REVISION SYSTEMS

---

## 12. Condensed Curriculum Roadmap

### 12.1 Time Estimates by Level

| Phase | Duration | Hours/Week | Total Hours | Key Milestone |
|-------|----------|------------|-------------|---------------|
| **Near N1** | 4 weeks | 12-15 | 48-60 | Reliable retrieval of core procedures |
| **Near N2** | 4 weeks | 12-15 | 48-60 | Interleaved practice fluency |
| **Near N3** | 4 weeks | 10-12 | 40-48 | Teaching capability; reconstruction mastery |
| **Urahara U1** | 4 weeks | 12-15 | 48-60 | Multi-path awareness; failure comfort |
| **Urahara U2** | 4 weeks | 12-15 | 48-60 | Pivot fluency; cross-domain transfer |
| **Urahara U3** | 4 weeks | 10-12 | 40-48 | Engineering integration; resource optimization |
| **L L1** | 4 weeks | 12-15 | 48-60 | Bayesian fluency; calibration habit |
| **L L2** | 4 weeks | 12-15 | 48-60 | Constraint propagation mastery |
| **L L3** | 4 weeks | 10-12 | 40-48 | Epistemic integration; adversarial thinking |

**Total: 36 weeks (9 months), 400-500 hours**

### 12.2 Weekly Structure Template

```
MONDAY:     New concept introduction + worked examples (2 hrs)
TUESDAY:   Guided practice with verification checkpoints (2 hrs)
WEDNESDAY: Interleaved review of previous material (1. 5 hrs)
THURSDAY:  Novel problem attempts + failure analysis (2 hrs)
FRIDAY:    Retrieval practice + spaced repetition (1.5 hrs)
WEEKEND:   Capstone problem or project (2-3 hrs)
```

### 12.3 Progress Milestones

**End of Near Level (Week 12):**
- [ ] Can reproduce any learned proof/derivation from memory
- [ ] Error rate <5% on practiced problem types
- [ ] Verification habit is automatic
- [ ] Maintains personal failure mode index <!-- ADDED -->

**End of Urahara Level (Week 24):**
- [ ] Generates 3+ solution paths for any problem
- [ ] Recovers from failures within 2 iterations
- [ ] Transfers methods across domains
- [ ] Articulates contingency paths before committing <!-- ADDED -->

**End of L Level (Week 36):**
- [ ] Calibration >80% (stated confidence matches accuracy)
- [ ] Maintains ranked hypotheses, not binary beliefs <!-- ADDED -->
- [ ] Explicitly surfaces assumptions <!-- ADDED -->
- [ ] Detects adversarial/deceptive scenarios
- [ ] Quantifies uncertainty in conclusions

---

## 13. Revision Drills Based on Retrieval Practice

### 13.1 Spaced Retrieval Schedule

| Time Since Learning | Retrieval Activity | Difficulty |
|--------------------|-------------------|------------|
| 1 day | Free recall of key concepts | Low |
| 3 days | Application to simple problems | Medium |
| 1 week | Application to moderate problems | Medium |
| 2 weeks | Mixed practice with related topics | High |
| 1 month | Novel application or teaching | High |
| 3 months | Comprehensive integration test | Very High |

### 13.2 Retrieval Drill Types

**Type 1: Free Recall**
```
Prompt: "What are the key steps in [procedure]?"
- Write everything you remember without looking
- Then check against source
- Note gaps and schedule targeted review
```

**Type 2: Cued Recall**
```
Prompt: "Given [partial information], complete [solution]"
- Fill in the blanks in derivations
- Complete proofs from intermediate steps
- Reconstruct from hints
```

**Type 3: Recognition + Justification**
```
Prompt: "Is this solution correct?  Why or why not?"
- Evaluate provided work
- Identify errors if present
- Explain reasoning for judgment
```

**Type 4: Application Transfer**
```
Prompt: "Apply [learned method] to [novel domain]"
- Identify abstract structure
- Map to new context
- Execute and verify
```

<!-- ADDED: Mid-solution verification drill -->
### 13.3 Mid-Solution Verification Drills (NEW)

**The Checkpoint Drill**
```
1. Begin solving a multi-step problem
2. At predetermined points (1/3, 2/3 through), STOP
3. Complete the verification checklist: 
   â–¡ Does intermediate result have correct units/type?
   â–¡ Does magnitude match intuition?
   â–¡ Does simple test case work?
   â–¡ What assumption am I currently relying on?
4. Only proceed after passing all checks
5. Track:  How often did checkpoints catch errors?
```

**The Assumption Audit Drill**
```
1. Solve a problem completely
2. Go back and list EVERY assumption (including "obvious" ones)
3. Categorize:  [Required by problem] vs. [Added by me]
4. For self-added assumptions:  Could relaxing them improve the solution?
5. Track: How many hidden assumptions did you find?
```

### 13.4 Failure Pattern Review

**Weekly Failure Review Protocol:**
```
1. Collect all errors from the week
2. Categorize by type (arithmetic, conceptual, procedural, oversight)
3. Identify the top 3 most frequent error types
4. For each: 
   - Name the failure pattern (retrieval tag)
   - Identify detection method
   - Design targeted drill
5. Add to personal Failure Mode Index
```

<!-- ADDED: Learning-rate tracking -->
### 13.5 Learning-Rate Tracking (NEW)

**Weekly Update Log:**
```markdown
## Week of [DATE]

### Beliefs That Changed
1. Before: I believed ________________
   After: I now believe ________________
   Evidence: ________________

2. Before: I estimated P(X) = ___%
   After: I estimate P(X) = ___%
   What caused update: ________________

### Update Speed Assessment
- Updates that happened quickly (good): ________________
- Updates that took too long (improve): ________________
- What made fast updates fast?  ________________

### Meta-Learning
- What I learned about how I learn: ________________
```

---

## 14. Externalization Strategy

### 14.1 Core Principle

> **Externalization Goal:** Reduce cognitive load by offloading low-value memory tasks, freeing mental resources for high-value reasoning. 

<!-- ADDED: Minimal friction emphasis -->
> **âš¡ Minimal Friction Requirement (NEW):** Any externalization system that takes more than 30 seconds to update will be abandoned.  Optimize for low friction, not comprehensiveness.

### 14.2 Recommended Tools (Minimal Set)

| Purpose | Tool Type | Example | Update Frequency |
|---------|-----------|---------|------------------|
| **Failure Mode Index** | Simple text file | Plain . txt or .md | After each session |
| **Spaced Repetition** | Flashcard system | Anki, RemNote | Daily review |
| **Problem Log** | Notebook/document | Paper or digital | After each problem |
| **Calibration Tracker** | Spreadsheet | Simple CSV | Weekly aggregation |

### 14.3 Failure Mode Index Structure

```markdown
# Personal Failure Mode Index

## Arithmetic/Calculation
- **Sign Propagation Error**: Lost negative sign through multiple steps
  - Detection: Check signs at key checkpoints
  - Retrieval cue: "Did I track the minus?"

- **Off-by-One**:  Fence-post errors in counting
  - Detection:  Verify with smallest concrete example
  - Retrieval cue:  "Posts or gaps?"

## Conceptual
- **Base Rate Neglect**: Ignored prior probability in Bayesian problems
  - Detection:  Any problem with rare events + tests
  - Retrieval cue: "What's the base rate?"

## Procedural
- **Premature Commitment**: Locked into first approach too early
  - Detection: No contingency plan articulated
  - Retrieval cue: "What's my backup?"

[Add entries as you discover personal patterns]
```

### 14.4 Problem Log Template

```markdown
## Problem:  [Name/ID]
**Date**:  YYYY-MM-DD
**Level**: Near / Urahara / L
**Domain**: Math / Physics / Logic / Engineering / Research

### Attempt Summary
- Time spent: ___ minutes
- Solved successfully: Yes / No / Partial
- Assistance needed: None / Hints / Full solution review

### Key Techniques Used
1. ________________
2. ________________

### Errors Made
- Error:  ________________
- Type: [Arithmetic / Conceptual / Procedural / Oversight]
- Retrieval tag: ________________

### Assumptions Made <!-- ADDED -->
- Explicit assumptions: ________________
- Implicit assumptions (discovered later): ________________

### Checkpoints <!-- ADDED -->
- Mid-solution verification performed: Yes / No
- Errors caught at checkpoints: ________________

### Reflection
- What I learned:  ________________
- What I would do differently: ________________
- Confidence in similar future problems: ___%
```

### 14.5 Calibration Tracking System

```markdown
## Calibration Log

| Date | Problem | Stated Confidence | Outcome | Calibration Error |
|------|---------|-------------------|---------|-------------------|
| 2024-01-15 | Bayes problem | 80% | Correct | 0 |
| 2024-01-15 | Constraint puzzle | 60% | Incorrect | -60 |
| 2024-01-16 | Derivation | 90% | Correct | 0 |

### Weekly Summary
- Mean stated confidence: ___%
- Actual accuracy: ___%
- Calibration gap: ___ percentage points
- Tendency:  Overconfident / Underconfident / Well-calibrated
```

<!-- ADDED:  Effort proportionality tracker -->
### 14.6 Cognitive Economy Tracker (NEW)

```markdown
## Effort Calibration Log

| Problem | Stakes | Effort Invested | Outcome | Retrospective |
|---------|--------|-----------------|---------|---------------|
| HW #3 | Low | High | Correct | Over-invested |
| Exam Q1 | High | High | Correct | Appropriate |
| Quick check | Low | Low | Correct | Appropriate |

### Patterns
- Do I over-invest in low-stakes problems? Yes / No
- Do I under-invest in high-stakes problems?  Yes / No
- Adjustment needed: ________________
```

---

## 15. Workflow Integration

### 15.1 Pre-Problem Ritual (2 minutes)

```
1. Read problem completely (30 sec)
2. Identify problem type and level (15 sec)
3. State assumptions explicitly (30 sec) <!-- ADDED -->
4.  Articulate primary approach (15 sec)
5. Articulate contingency approach (15 sec) <!-- ADDED -->
6. Set checkpoint locations for long problems (15 sec) <!-- ADDED -->
```

### 15.2 Mid-Problem Checkpoints

```
At 1/3 and 2/3 marks:
â–¡ Verify intermediate results (units, magnitude, sign)
â–¡ Confirm still on viable path
â–¡ Check:  Am I in a known failure mode?
â–¡ Update:  Is my confidence calibrated to progress?  <!-- ADDED -->
```

### 15.3 Post-Problem Ritual (3 minutes)

```
1. Verify final answer (sanity checks) (30 sec)
2. Identify errors made and assign retrieval tags (45 sec)
3. Complete assumption audit (30 sec) <!-- ADDED -->
4. State confidence in correctness:  ___% (15 sec)
5. Log in problem journal if significant (60 sec)
6. Update failure mode index if new pattern (30 sec) <!-- ADDED -->
```

### 15.4 Weekly Integration Session (30 minutes)

```
1. Review failure mode index additions (5 min)
2. Calculate calibration metrics (5 min)
3.  Identify top 3 error patterns (5 min)
4. Design targeted drills for patterns (10 min)
5. Update spaced repetition items (5 min)
6. Learning-rate reflection:  What updated?  What should have updated faster? (5 min) <!-- ADDED -->
```

---

# PART VI:  INTEGRATION SUMMARY

---

## 16. What Was Added & Why

### Summary of Augmentations

This document was augmented with six trait clusters that were previously implicit or missing, each mapped to appropriate locations within the existing Near â†’ Urahara â†’ L framework:

**1. Epistemic Hygiene Enhancements** were integrated primarily into L-level materials, where probabilistic reasoning demands explicit assumption surfacing, confidence quantification, and hypothesis ranking.  These additions appear as checkpoint prompts, reflection template additions, and the "Epistemic Hygiene Protocol" callout.

**2. Memory Indexing & Retrieval Control** was embedded throughout Near-level materials, where the foundation of reliable retrieval is built.  The "retrieval tag" column was added to all pitfall tables, and the "Failure Mode Index" externalization structure was introduced.  This recognizes that expertise involves organized access, not just stored content.

**3. Error-First & Failure-Aware Design** was integrated primarily into Urahara-level materials, aligning with the adaptive engineering orientation. Mid-solution verification checkpoints, contingency path requirements, and the error-first mindset callout were added.  These changes acknowledge that good thinking assumes fallibility.

**4. Rule-Space & Constraint Awareness** was added as a new Peak Brilliance subsection (3. 2), emphasizing that advanced problem-solving treats problems as formal systems with exploitable structureâ€”but only after correctness is secured. The "Assumption Audit" drill reinforces this. 

**5. Learning-Rate Optimization** was woven into L-level reflection templates and the weekly integration workflow, emphasizing that cognitive development should be measured by speed of model update, not just eventual correctness.

**6. Cognitive Economy** was added to the Guiding Principles table and integrated as checkpoint prompts throughout, particularly in capstone activities.  This prevents over-investment in low-stakes problems and preserves resources for high-stakes reasoning.

All additions were designed to feel like natural completions of existing structures rather than external grafts, using consistent formatting (callout boxes, table extensions, reflection template additions) and preserving the document's humane, reliability-first philosophy.

---

## 17. Deviation Checklist

### Confirmation of Adherence to Original Goals

| Constraint | Adhered?  | Evidence |
|------------|----------|----------|
| âŒ No full rewrite or restructuring | âœ… Yes | Original structure preserved; additions are annotations, callouts, table extensions |
| âŒ No new archetypes | âœ… Yes | Near â†’ Urahara â†’ L progression unchanged |
| âŒ No difficulty increase for its own sake | âœ… Yes | Additions focus on reliability and hygiene, not complexity |
| âœ… Only augment, embed, annotate | âœ… Yes | All additions are clearly marked with <!-- ADDED --> or "(NEW)" |
| âœ… Preserve humane, reliability-first philosophy | âœ… Yes | Cognitive economy and assumption hygiene support sustainable practice |

### Trait Cluster Integration Verification

| Trait Cluster | Integrated Location(s) | Integration Type |
|---------------|----------------------|------------------|
| Epistemic Hygiene | L1-L3 phases; Problem L1, L2, L3; Meta-reasoning rubrics | Checkpoints, prompts, protocol box |
| Memory Indexing | Near competencies; All pitfall tables; Section 14.3 | Table column, index structure, retrieval tags |
| Error-First Design | Urahara definition; U1-U3 phases; Problem U1, U2 | Callout box, contingency requirement, named tags |
| Rule-Space Awareness | Section 3.2 (new); U3 capstone | New subsection, assumption audit drill |
| Learning-Rate Optimization | Section 3.3; L3 reflection; Section 13.5 | Annotation, template addition, tracking log |
| Cognitive Economy | Section 1 principles; N3, U3 capstones; Section 14.6 | Table row, checkpoint prompts, tracker |

---

## 18. Quick Reference Card

### Pre-Problem Checklist (30 seconds)

```
â–¡ What type of problem is this?  (Near/Urahara/L)
â–¡ What assumptions am I making? 
â–¡ What is my primary approach? 
â–¡ What is my contingency if that fails?
â–¡ Where will I place verification checkpoints?
â–¡ Is this high-stakes (invest fully) or low-stakes (good-enough)?
```

### Mid-Solution Verification (15 seconds)

```
â–¡ Correct units/type/sign?
â–¡ Magnitude matches intuition? 
â–¡ Am I in a known failure mode? 
â–¡ Confidence still calibrated? 
```

### Post-Problem Reflection (60 seconds)

```
â–¡ Answer verified?
â–¡ Errors tagged with retrieval labels?
â–¡ Assumptions audited?
â–¡ Confidence stated? 
â–¡ Learning logged if significant?
```

### The Five Questions of L-Level Reasoning

```
1. What are my assumptions? (explicit + implicit)
2. What is my confidence?  (as a percentage)
3. How are my hypotheses ranked? (H1 > H2 > H3)
4. What would change my mind? (update triggers)
5. What changed in my understanding? (learning-rate)
```

---

*End of Augmented Cognitive Capability Framework*
